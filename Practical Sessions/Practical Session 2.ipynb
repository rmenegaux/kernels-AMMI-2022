{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session 2\n",
    "### Kernel Methods for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Yunlong Jiao / Romain Menegaux, 21 June 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model as lm\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement (naive) solvers to Ridge Regression, Weighted Ridge Regression and Logistic Ridge Regression (using Iteratively Reweighted Least Squares). See notes for the mathematical derivation.\n",
    "2. Simulate some toy data to check if our solvers give correct solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ridge Regression (RR)\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$, solve\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n} \\|y - X \\beta\\|^2 + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression (RR)\n",
    "def solveRR(y, X, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    # Hint:\n",
    "    # beta = np.linalg.solve(A, b)\n",
    "    # Finds solution to the linear system Ax = b\n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of our Ridge Regression solver:\n",
    "#### One-dimensional data\n",
    "(for visualization purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 100\n",
    "X = np.random.rand(n)\n",
    "beta_star = 0.8\n",
    "y = X * beta_star + 0.1 * np.random.normal(0, 1, n)\n",
    "\n",
    "lam = .1\n",
    "beta_hat = solveRR(y, X[:, None], lam)\n",
    "\n",
    "print(beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title('Ridge regression, lambda = {}'.format(lam))\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, X * beta_hat, c='r')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on p-dimensional data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy data\n",
    "np.random.seed(42) # for reproducibility\n",
    "n = 100 # number of samples\n",
    "p = 10 # number of features\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "X = sklearn.preprocessing.scale(X) # scale to 0 mean and 1 standard deviation\n",
    "beta_star = np.random.normal(0, 1, p)\n",
    "# y = X * beta + noise\n",
    "y = X.dot(beta_star) + 0.2 * np.random.normal(0, 1, n) \n",
    "\n",
    "def compare(beta1, beta2):\n",
    "    print('''\n",
    "Our solver:\n",
    "{}\n",
    "Scikit-learn:\n",
    "{}\n",
    "\n",
    "Difference between the two:\n",
    "{:.1e}\n",
    "        '''.format(beta1.round(2), beta2.round(2), float(np.sum((beta1-beta2)**2)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.1\n",
    "\n",
    "# Our solver\n",
    "beta1 = solveRR(y, X, lam)\n",
    "\n",
    "# Python solver\n",
    "alpha = lam * X.shape[0]\n",
    "model = lm.Ridge(alpha=alpha, fit_intercept=False, normalize=False)\n",
    "beta2 = model.fit(X, y).coef_\n",
    "\n",
    "# Check\n",
    "compare(beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Weighted Ridge Regression (WRR)\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$, and weights $w \\in \\mathbb{R}^n_+$, solve\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n} \\sum_{i=1}^n w_i (y_i - \\beta^\\top x_i)^2 + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$\n",
    "\n",
    "**Goal:** Express the objective as a regular Ridge Regression (RR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Ridge Regression (WRR)\n",
    "def solveWRR(y, X, w, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == len(w) == n)\n",
    "\n",
    "    # Hint:\n",
    "    # Find y1 and X1 such that:\n",
    "    # beta = solveRR(y1, X1, lam)\n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it out:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.1\n",
    "w = np.random.rand(len(y))\n",
    "\n",
    "# Our solver\n",
    "beta1 = solveWRR(y, X, w, lam)\n",
    "\n",
    "# Python solver\n",
    "alpha = lam * X.shape[0]\n",
    "model = lm.Ridge(alpha=alpha, fit_intercept=False, normalize=False)\n",
    "beta2 = model.fit(X, y, sample_weight=w).coef_\n",
    "\n",
    "# Check\n",
    "compare(beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Logistic Ridge Regression (LRR)\n",
    "----\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\{-1,+1\\}^n$, solve\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n} \\sum_{i=1}^n \\log (1+e^{-y_i \\beta^\\top x_i}) + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ be the sigmoid function.\n",
    "\n",
    "**Exercise:** Compute $\\sigma'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewriting $J$:\n",
    "$$\n",
    "J(\\beta) = - \\frac{1}{n} \\sum_{i=1}^n {\\log(\\sigma(y_i\\beta^\\top x_i))} + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$\n",
    "\n",
    "**Exercise:** Compute its gradient $\\nabla J$, and its Hessian $\\nabla^2 J$\n",
    "$$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "- Initialize $\\beta^{old} = \\beta_0$\n",
    "- Repeat until convergence: $$\n",
    "\\beta^{new} \\leftarrow \\beta^{old} - h \\nabla J(\\beta^{old})\n",
    "$$\n",
    "where $h$ is the step size (learning rate)\n",
    "\n",
    "Under some conditions on $J$ (for example $J$ is strongly convex), and with an appropriate step-size $h$, this algorithm converges to a point $\\beta^*$ such that $\\nabla J(\\beta^*) = 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Logistic Ridge Regression with Gradient Descent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Ridge Regression (LRR) with gradient descent (GD)\n",
    "def solveLRR_gradient(y, X, lam, max_iter=500, eps=1e-3):\n",
    "    '''\n",
    "    lam: Regularization parameter\n",
    "    max_iter: Max number of iterations of gradient descent\n",
    "    eps: Tolerance for stopping criteria \n",
    "    '''\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "            \n",
    "    # for i in range(max_iter):\n",
    "    #     ...\n",
    "    #         \n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it out:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin = np.sign(y) # Binarize targets\n",
    "lam = 0.1\n",
    "\n",
    "# Our solver\n",
    "beta_gradient = solveLRR_gradient(y_bin, X, lam)\n",
    "\n",
    "# Python solver\n",
    "alpha = 2 * lam * X.shape[0]\n",
    "model = lm.LogisticRegression(C=1/alpha, fit_intercept=False)\n",
    "beta_sklearn = model.fit(X, y_bin).coef_\n",
    "\n",
    "# Check\n",
    "compare(beta_gradient, beta_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for optimal $\\beta$ using Newton-Raphson\n",
    "$$\n",
    "\\beta^{new} \\leftarrow \\beta^{old} - \\left(\\nabla^2 J(\\beta^{old})\\right)^{-1} \\nabla J(\\beta^{old})\n",
    "$$\n",
    "\n",
    "Show that each step is equivalent to solving a weighted ridge regression (WRR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Quadratic approximation to $J$</font>:\n",
    "\n",
    "$$\n",
    "J(\\beta) \\approx J_q(\\beta) = J(\\beta^{old}) + (\\beta - \\beta^{old})^\\top \\nabla J(\\beta^{old}) + \\frac{1}{2} (\\beta - \\beta^{old})^\\top \\nabla^2 J(\\beta^{old}) (\\beta - \\beta^{old})\n",
    "$$\n",
    "\n",
    "- **Step 1**: Show that $$\\min_\\beta J_q(\\beta) = \\beta^{new}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 2**: Show that $J_q$ is a WRR objective -- find $W$, $z$ such that:\n",
    "<font color='green'>\n",
    "$$\n",
    "2J_q(\\beta) = (z - X\\beta)^\\top W (z - X\\beta) + 2\\lambda \\|\\beta\\|^2 + \\mathrm{C}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Logistic Ridge Regression with Newton-Raphson method\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Ridge Regression with Newton-Raphson\n",
    "def solveLRR_newton(y, X, lam, max_iter=500, eps=1e-3):\n",
    "    '''\n",
    "    lam: Regularization parameter\n",
    "    max_iter: Max number of iterations of gradient descent\n",
    "    eps: Tolerance for stopping criteria\n",
    "    '''\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "            \n",
    "    # Hint: Use IRLS\n",
    "    # for i in range(max_iter):\n",
    "    #     ...\n",
    "    #     beta = solveWRR(z, X, w, 2*lam)    \n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our solver\n",
    "beta_newton = solveLRR_newton(y_bin, X, lam)\n",
    "\n",
    "compare(beta_newton, beta_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Mini Data Challenge\n",
    "\n",
    "We will try to predict whether patients have breast cancer.\n",
    "\n",
    "We use scikit-learn's [breast cancer dataset](https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset)\n",
    "\n",
    "30 features, 569 samples, 2 labels ('malignant' or 'benign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and split into training / validation sets\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = load_breast_cancer(return_X_y=True)\n",
    "data = load_breast_cancer()\n",
    "X, y = data['data'], data['target']\n",
    "y = 2*y - 1 # transform from {0, 1} to {-1, 1}\n",
    "\n",
    "# Hint: Scaling can be important\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model and compute its parameters\n",
    "lam = 0.01\n",
    "beta = solveLRR_newton(y_train, X_train, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities and classes\n",
    "# probas_pred = ?\n",
    "# y_pred = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "print(\"Our model's performance:\")\n",
    "print('Accuracy: {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('AUC: {:.2%}'.format(roc_auc_score(y_test, probas_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the feature importances\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "features = data['feature_names']\n",
    "ax.barh(features, beta)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
