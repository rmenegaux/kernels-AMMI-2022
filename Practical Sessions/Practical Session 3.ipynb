{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session 3\n",
    "## Kernel Methods for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Yunlong Jiao / Romain Menegaux, 20 May 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* [Part I - Basics](#part1)\n",
    "    1. [Generating data](#generating-data)\n",
    "    2. [SVM scikit-learn implementation](#scikit-implementation)\n",
    "    3. [Plotting the results](#plotting-margins)\n",
    "* [Part II - Tuning C](#part2)\n",
    "    1. [Choosing C by cross-validation](#cross-validation)\n",
    "    2. [Influence of C on performance](#best-C)\n",
    "    3. [Interactive Plot](#interactive-plot)\n",
    "* [Part III - Custom Implementation using Quadratic Programming](#part3)\n",
    "    1. [Hard Margin](#hard-margin)\n",
    "       * [Primal](#hard-margin-primal)\n",
    "       * [Dual](#hard-margin-dual)\n",
    "       * [Getting w and b from dual solution](#primal-from-dual-hard)\n",
    "       * [Non separable data](#non-separable)\n",
    "    2. [Soft Margin](#soft-margin)\n",
    "       * [Primal](#soft-margin-primal)\n",
    "       * [Dual](#soft-margin-dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "solver='cvxopt'\n",
    "# solver = 'quadprog'\n",
    "\n",
    "if solver == 'cvxopt':\n",
    "    try:\n",
    "        import cvxopt\n",
    "    except:\n",
    "        try:\n",
    "            # If conda install fails for you, try pip install:\n",
    "            # !{sys.executable} -m pip install cvxopt\n",
    "            !conda install --yes --prefix {sys.prefix} cvxopt\n",
    "            import cvxopt\n",
    "        except:\n",
    "            raise ImportError('Could not import or conda install cvxopt. You could try installing with pip. Or try solver=quadprog')\n",
    "\n",
    "if solver == 'quadprog':\n",
    "    try:\n",
    "        import quadprog\n",
    "    except:\n",
    "        try:\n",
    "            !conda install --yes --prefix {sys.prefix} -c omnia quadprog\n",
    "            # If conda install fails for you, try pip install:\n",
    "            # !{sys.executable} -m pip install quadprog\n",
    "            import quadprog\n",
    "        except:\n",
    "            raise ImportError('Could not import or conda install quadprog. You could try installing with pip. Or try solver=cvxopt')\n",
    "            \n",
    "print('Using solver `{}`'.format(solver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Part I - Basics <a name=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data <a name=\"generating-data\"></a> \n",
    "- **Generate 20 binary classification samples** (10 from each class $y = 1$ and $y = −1$).\n",
    "\n",
    "Use $\\mathbf{x} \\in \\mathbb{R}^2$, for visualization purpose, where each element $x \\sim \\mathcal{N}(\\mu_y,\\sigma^2), \\mu_1 = 0, \\mu_{-1}=1$.\n",
    "\n",
    "- **Generate 1000 separate testing samples from the same distribution**\n",
    "- **Visualize training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "np.random.seed(54321)\n",
    "p = 2\n",
    "\n",
    "def generate_Xy(n_samples, p=2, sigma=.2):\n",
    "    # X of shape (n_samples, p)\n",
    "    # y vector of length n_samples\n",
    "    return X, y\n",
    "\n",
    "# Training data\n",
    "X_train, y_train = generate_Xy(20)\n",
    "\n",
    "# Testing data\n",
    "X_test, y_test = generate_Xy(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training data\n",
    "def plot_data(X, y, title=''):\n",
    "    plt.figure(figsize=(8,7))\n",
    "    plt.title(title)\n",
    "    # use plt.scatter to plot data\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.show()\n",
    "    \n",
    "plot_data(X_train, y_train, title='Training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_test, y_test, title='Test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM scikit-learn implementation <a name=\"scikit-implementation\"></a>\n",
    "**Train a linear SVM** with parameter $C = 10$ on the training set, using the function `sklearn.svm.LinearSVC`. Inspect the resulting object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Parameters\n",
    "C = 10\n",
    "\n",
    "# Create the model\n",
    "clf = 'LinearSVC object, fill me in'\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "clf.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction error\n",
    "def error(y_pred, y_true):\n",
    "    e = (y_pred != y_true).mean()\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = clf.coef_[0]\n",
    "b = clf.intercept_[0]\n",
    "print('Model parameters:')\n",
    "print('w: {}'.format(w))\n",
    "print('b: {}'.format(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training error\n",
    "y_fit = clf.predict(X_train)\n",
    "err_train = error(y_fit, y_train)\n",
    "print('Training error = {:.1%}'.format(err_train))\n",
    "\n",
    "# Testing error\n",
    "y_pred = clf.predict(X_test)\n",
    "err_test = error(y_pred, y_test)\n",
    "print('Testing error = {:.1%}'.format(err_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder\n",
    "\n",
    "SVM as a model:\n",
    "$\\hat{y} = sign(w^\\top x + b)$\n",
    "\n",
    "Goal: find the best possible $w$ and $b$\n",
    "$$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our implementation of `clf.predict`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What clf.predict does under the hood:\n",
    "y_fit = 'fill me in, using X_train, w and b'\n",
    "err_train = error(y_fit, y_train)\n",
    "print('Training error = {:.1%}'.format(err_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the error so low?**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Plot the points and the decision boundaries of a linear SVM <a name=\"plotting-margins\"></a>\n",
    "Recall: \n",
    "the boundary (separating hyperplane) is the set of points for which $f(x) = w^T x + b = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize training data and separating hyperplane\n",
    "def compute_margins(b, w1, w2, x1):\n",
    "    '''\n",
    "    returns x2, x2_up, x2_low such that:\n",
    "        - f(x2) = 0\n",
    "        - f(x2_up) = 1\n",
    "        - f(x2_low) = -1\n",
    "    '''\n",
    "    x2 = ...\n",
    "    x2_up = ...\n",
    "    x2_low = ...\n",
    "    \n",
    "    return x2, x2_up, x2_low\n",
    "\n",
    "def plot_points_with_margin(X, y, w, b):\n",
    "    plt.figure(figsize=(8,7))\n",
    "\n",
    "    # Training data\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "\n",
    "    # Separating hyperplane and decision boundaries\n",
    "    x1 = np.linspace(X[:,0].min(), X[:,0].max(), 100)\n",
    "    w1, w2 = w\n",
    "    x2, x2_up, x2_low = compute_margins(b, w1, w2, x1)\n",
    "\n",
    "    plt.plot(x1, x2, 'b')\n",
    "    plt.plot(x1, x2_up, 'b--')\n",
    "    plt.plot(x1, x2_low, 'b--')\n",
    "\n",
    "    # Plot\n",
    "    plt.show()\n",
    "    \n",
    "w = clf.coef_[0]\n",
    "b = clf.intercept_[0]\n",
    "plot_points_with_margin(X_test, y_test, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Tuning C <a name=\"part2\"></a>\n",
    "- Try different values of $C$, and visualize the effect of $C$ on the margin and number of support vectors.\n",
    "- Train a linear SVM with values of $C$ between $10^{−3}$ and $1$. Plot and analyze the training and testing classification errors as a function of $C$. Question: Why can't we overfit more?\n",
    "- Choose $C$ by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make the data less separable\n",
    "# Training data\n",
    "X_train2, y_train2 = generate_Xy(50, sigma=.5)\n",
    "# Testing data\n",
    "X_test2, y_test2 = generate_Xy(1000, sigma=.5)\n",
    "\n",
    "plot_data(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing C by cross-validation <a name=\"cross-validation\"></a>\n",
    "\n",
    "Find the best $C$ using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross validation\n",
    "C_candidates = 10 ** np.linspace(-3, 2, 100) # Possible values of C we want to test\n",
    "score_candidates = np.zeros(len(C_candidates)) # Cross-validation scores for each of the Cs\n",
    "\n",
    "for i, C in enumerate(C_candidates):\n",
    "    clf = sklearn.svm.LinearSVC(penalty='l2',loss='hinge', C=C)\n",
    "    scores = cross_val_score(clf, X_train2, y_train2, cv=5, scoring='accuracy')\n",
    "    score_candidates[i] = scores.mean()\n",
    "    \n",
    "# Find the index of the best C:\n",
    "best_index = ...\n",
    "best_C = C_candidates[best_index]\n",
    "\n",
    "# Training with cross-validated C\n",
    "clf_best = sklearn.svm.LinearSVC(penalty='l2', loss='hinge', C=best_C)\n",
    "clf_best.fit(X_train2, y_train2)\n",
    "\n",
    "print('Results with optimal C')\n",
    "print('----------------------')\n",
    "# Training error\n",
    "y_fit = clf_best.predict(X_train2)\n",
    "err_train = error(y_fit, y_train2)\n",
    "print('Training error = {:.1%}'.format(err_train))\n",
    "\n",
    "# Testing error\n",
    "y_pred = clf_best.predict(X_test2)\n",
    "err_test = error(y_pred, y_test2)\n",
    "print('Testing error = {:.1%}'.format(err_test))\n",
    "\n",
    "plot_points_with_margin(X_train2, y_train2, clf_best.coef_[0], clf_best.intercept_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of $C$ on performance <a name=\"best-C\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test error curve\n",
    "err_train = np.zeros(len(C_candidates))\n",
    "err_test = np.zeros(len(C_candidates))\n",
    "\n",
    "# OPTIONAL\n",
    "# We also store the model coefficients to plot the decision boundaries\n",
    "b = np.zeros_like(C_candidates)\n",
    "w = np.zeros((len(C_candidates), p))\n",
    "\n",
    "for i, C in enumerate(C_candidates):\n",
    "    # print('C = ', C)\n",
    "    clf = sklearn.svm.LinearSVC(penalty='l2',loss='hinge',C=C)\n",
    "    clf.fit(X_train2, y_train2)\n",
    "    \n",
    "    # Store model coefs\n",
    "    b[i] = clf.intercept_[0]\n",
    "    w[i] = clf.coef_[0]\n",
    "\n",
    "    # Training error\n",
    "    y_fit = clf.predict(X_train2)\n",
    "    err_train[i] = error(y_fit, y_train2)\n",
    "    # print('Training error = ', err_train[i])\n",
    "    \n",
    "    # Testing error\n",
    "    y_pred = clf.predict(X_test2)\n",
    "    err_test[i] = error(y_pred, y_test2)\n",
    "    # print('Testing error = ', err_test[i])\n",
    "\n",
    "# Plot training and testing error as a function of C\n",
    "plt.figure()\n",
    "plt.plot(C_candidates, err_train, label='train')\n",
    "plt.xscale('log')\n",
    "plt.plot(C_candidates, err_test, label='test')\n",
    "plt.axvline(best_C, linewidth=1, color='g', label='cross-val C')\n",
    "plt.legend()\n",
    "plt.scatter(best_C, err_train[best_index], marker='x', label='train')\n",
    "plt.scatter(best_C, err_test[best_index], marker='x', label='test')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive plot <a name=\"interactive-plot\"></a>\n",
    "\n",
    "This cell requires the packages [bqplot](https://github.com/bqplot/bqplot) and [ipywidgets](https://ipywidgets.readthedocs.io/en/latest/user_install.html) to be installed. (See installation instructions, then refresh the page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental (not guaranteed to work for everyone):\n",
    "\n",
    "# Conda installation from the notebook: un-comment the following 2 lines:\n",
    "# !conda install --yes --prefix {sys.prefix} -c conda-forge bqplot\n",
    "# !conda install --yes --prefix {sys.prefix} ipywidgets\n",
    "\n",
    "# If conda install fails for you, try pip install, un-comment the following 3 lines:\n",
    "# !{sys.executable} -m pip install bqplot\n",
    "# !jupyter nbextension enable --py --prefix {sys.prefix} bqplot\n",
    "# !{sys.executable} -m pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import bqplot.pyplot as bqp\n",
    "from ipywidgets import IntSlider, HTML, HBox, VBox\n",
    "\n",
    "x1 = np.linspace(X_test2[:,0].min(), X_test2[:,0].max(), 100)\n",
    "\n",
    "slider_C = IntSlider(min=0, max=len(CL)-1, value=len(CL)/2,\n",
    "                     description='Choose C:', readout=False)\n",
    "readout = HTML()\n",
    "fig = bqp.figure(min_aspect_ratio=1.1, max_aspect_ratio=1.1,\n",
    "           layout={'min_width': '500px', 'min_height': '500px'})\n",
    "scat = bqp.scatter(X_test2[::2, 0], X_test2[::2, 1],\n",
    "                   colors=['orange' if y == 1 else 'purple' for y in y_test2[::2]],\n",
    "                   default_size=40, opacities=[0.7])\n",
    "                   #axes_options={'color': None})\n",
    "sep = bqp.plot(x1, np.zeros_like(x1), preserve_domain={'x': True, 'y': True})\n",
    "margin_up = bqp.plot(x1, np.zeros_like(x1), '--', preserve_domain={'x': True, 'y': True})\n",
    "margin_down = bqp.plot(x1, np.zeros_like(x1), '--', preserve_domain={'x': True, 'y': True})\n",
    "\n",
    "\n",
    "def update_margins(*args):\n",
    "    i = slider_C.value\n",
    "    sep.y, margin_up.y, margin_down.y = compute_margins(b[i], w[i, 0], w[i, 1], sep.x)\n",
    "    readout.value = 'C = {:.3f}'.format(CL[i])\n",
    "\n",
    "slider_C.observe(update_margins)\n",
    "update_margins()\n",
    "\n",
    "#bqp.show()\n",
    "VBox([fig, HBox([slider_C, readout])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Part III - Custom SVM implementation with quadratic programming<a name=\"part3\"></a>\n",
    "We will use a quadratic program (QP) solver `cvxopt` to find our own solution to SVM\n",
    "\n",
    "```\n",
    "cvxopt.solvers.qp(P, q[, G, h[, A, b]])\n",
    "```\n",
    "solves the quadratic program\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_x & \\, \\frac{1}{2}x^\\top P x + q^\\top x \\\\\n",
    "\\mathrm{s.t. } \\, & Gx \\leq h \\\\\n",
    "& Ax = b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $P, q$ define the objective\n",
    "- $G, h$ are all the inequality constraints\n",
    "- $A, b$ are all the equality constraints\n",
    "\n",
    "**Goal: Find $P$, $q$, $G$, $h$, $A$ and $b$ for SVM**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to look at this, this is just to adapt our matrices\n",
    "# to the solver being used\n",
    "\n",
    "def quadprog_solve_qp(P, q, G=None, h=None, A=None, b=None):\n",
    "    qp_G = .5 * (P + P.T)   # make sure P is symmetric\n",
    "    qp_a = -q\n",
    "    if A is not None:\n",
    "        qp_C = -np.vstack([A, G]).T\n",
    "        qp_b = -np.hstack([b, h])\n",
    "        meq = A.shape[0]\n",
    "    else:  # no equality constraint\n",
    "        qp_C = - G.T\n",
    "        qp_b = - h\n",
    "        meq = 0\n",
    "    return quadprog.solve_qp(qp_G, qp_a, qp_C, qp_b, meq)[0]\n",
    "\n",
    "def cvxopt_qp(P, q, G, h, A, b):\n",
    "    P = .5 * (P + P.T)\n",
    "    cvx_matrices = [\n",
    "        cvxopt.matrix(M) if M is not None else None for M in [P, q, G, h, A, b] \n",
    "    ]\n",
    "    solution = cvxopt.solvers.qp(*cvx_matrices)\n",
    "    return np.array(solution['x']).flatten()\n",
    "\n",
    "solve_qp = {'quadprog': quadprog_solve_qp, 'cvxopt': cvxopt_qp}[solver]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Margin SVM <a name=\"hard-margin\"></a>\n",
    "(See slide 83)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal problem (hard margin) <a name=\"hard-margin-primal\"></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w, b} & \\, \\frac{1}{2}w^\\top w \\\\\n",
    "\\mathrm{s.t. } \\, & y_i x_i^\\top w + y_i b \\geq 1 \\,\\,\\,, \\forall i \\in [1, n]\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step: what is $x$ in our case, with the slides notations?\n",
    "\n",
    "$x = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**We set $X_y = \\mathrm{diag}(y)X = (y_1x_1, ..., y_nx_n)^\\top \\in \\mathbb{R}^{n\\times p}$**\n",
    "\n",
    "$X_y$ is simply $X$ with its rows $i$ multiplied by $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard margin SVM\n",
    "\n",
    "**Primal**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w, b} & \\, \\frac{1}{2}w^\\top w \\\\\n",
    "\\mathrm{s.t. } \\, & y_i x_i^\\top w + y_i b \\geq 1\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<font color='red'> Watch out for the signs in the constraints! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_primal_hard_to_qp(X, y):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    Xy = np.diag(y).dot(X)\n",
    "    # Primal formulation, hard margin\n",
    "    diag_P = np.zeros(p + 1) # correct this!\n",
    "    # As a regularization, we add epsilon * identity to P\n",
    "    eps = 1e-12\n",
    "    diag_P += eps\n",
    "    P = np.diag(diag_P)\n",
    "    \n",
    "    q = \n",
    "    G = \n",
    "    h = \n",
    "    A = None\n",
    "    b = None\n",
    "    \n",
    "    return P, q, G, h, A, b\n",
    "\n",
    "x = solve_qp(*svm_primal_hard_to_qp(X_train, y_train))\n",
    "n, p = X_train.shape\n",
    "w, b = x[:-1], x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points_with_margin(X_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual: <a name=\"hard-margin-dual\"></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_\\alpha & \\, \\mathrm{1}^\\top\\alpha - \\frac{1}{2}\\alpha^\\top X_y X_y^\\top \\alpha \\\\\n",
    "\\mathrm{s.t. } \\, & \\alpha \\geq 0 \\\\\n",
    "& y^\\top\\alpha = 0\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "You can verify that:\n",
    "$$\n",
    "L(\\alpha) =  -\\frac{1}{2}\\sum_{i, j} \\alpha_i \\alpha_j x_i^\\top x_j y_i y_j + \\sum_i \\alpha_i = -\\frac{1}{2} \\alpha^\\top X_y X_y^\\top \\alpha + \\mathbf{1}^\\top\\alpha\n",
    "$$\n",
    "\n",
    "Then implement it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_dual_hard_to_qp(X, y):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    Xy = np.diag(y).dot(X)\n",
    "    # Dual formulation, hard margin\n",
    "    P = \n",
    "\n",
    "    # As a regularization, we add epsilon * identity to P\n",
    "    eps = 1e-12\n",
    "    P += eps * np.eye(n)\n",
    "    \n",
    "    q =\n",
    "    G =\n",
    "    h =\n",
    "    A =\n",
    "\n",
    "    b =\n",
    "\n",
    "    return P, q, G, h, A, b\n",
    "\n",
    "alphas = solve_qp(*svm_dual_hard_to_qp(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we find $w$ and $b$ from $\\alpha$? $$$$<a name=\"primal-from-dual-hard\"></a>\n",
    "$$$$\n",
    "\n",
    "Answer is in slide 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primal_from_dual(alpha, X, y, hard_margin=False, C=None, tol=1e-3):\n",
    "    # w parameter in vectorized form\n",
    "    w = \n",
    "    \n",
    "    # sv = Support vectors!\n",
    "    # Indices of points (support vectors) that lie exactly on the margin\n",
    "    # Filter out points with alpha == 0\n",
    "    sv = \n",
    "    # If soft margin, also filter out points with alpha == C\n",
    "    if not hard_margin:\n",
    "        if C is None:\n",
    "            raise ValueError('C must be defined in soft margin mode')\n",
    "        sv = np.logical_and(sv, ?)\n",
    "    b = \n",
    "    \n",
    "    #Display results\n",
    "    print('Alphas = {}'.format(alpha[sv]))\n",
    "    print('Number of support vectors = {}'.format(sv.sum()))\n",
    "    print('w = {}'.format(w))\n",
    "    print('b = {}'.format(b))\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "w, b = get_primal_from_dual(alphas, X_train, y_train, hard_margin=True)\n",
    "plot_points_with_margin(X_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does not work if data is not linearly separable! <a name=\"non-separable\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_non_separable, y_non_separable = generate_Xy(20, sigma=2)\n",
    "solve_qp(*svm_primal_hard_to_qp(X_non_separable, y_non_separable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft margin SVM <a name=\"soft-margin\"></a>\n",
    "(slides 89, 91)\n",
    "\n",
    "### Primal (soft margin): <a name=\"soft-margin-primal\"></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w, b, \\xi} & \\, \\frac{1}{2}w^\\top w + C \\mathbf{1}^\\top \\xi \\\\\n",
    "\\mathrm{s.t. } \\, & \\xi \\geq 0 \\\\\n",
    "& y_i x_i^\\top w + y_i b + \\xi_i\\geq 1\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def svm_primal_soft_to_qp(X, y, C=1):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    Xy = np.diag(y).dot(X)\n",
    "    # Primal formulation, soft margin\n",
    "    diag_P = \n",
    "    # As a regularization, we add epsilon * identity to P\n",
    "    eps = 1e-12\n",
    "    diag_P += eps\n",
    "    P = np.diag(diag_P)\n",
    "    \n",
    "    q = \n",
    "    G =\n",
    "    h =\n",
    "    A = None\n",
    "    b = None\n",
    "    return P, q, G, h, A, b\n",
    "\n",
    "coefs = solve_qp(*svm_primal_soft_to_qp(X_train, y_train, C=C))\n",
    "n, p = X_train.shape\n",
    "w, b, e = coefs[:p], coefs[p], coefs[(p+1):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_points_with_margin(X_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual (soft margin): <a name=\"soft-margin-dual\"></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_\\alpha & \\, \\mathrm{1}^\\top\\alpha -\\frac{1}{2}\\alpha^\\top X_y^T X_y \\alpha \\\\\n",
    "\\mathrm{s.t. } \\, & \\alpha \\geq 0 \\\\\n",
    "& \\alpha \\leq C \\\\\n",
    "& y^\\top\\alpha = 0\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_dual_soft_to_qp(X, y, C=1):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    Xy = np.diag(y).dot(X)\n",
    "    # Dual formulation, soft margin\n",
    "    P =\n",
    "    # As a regularization, we add epsilon * identity to P\n",
    "    eps = 1e-12\n",
    "    P += eps * np.eye(n)\n",
    "    q =\n",
    "    G =\n",
    "    h =\n",
    "    A =\n",
    "    b =\n",
    "    return P, q, G, h, A, b\n",
    "\n",
    "C = 10\n",
    "alphas = solve_qp(*svm_dual_soft_to_qp(X_train, y_train, C=C))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Finding $w$ and $b$ from $\\alpha$ $$$$<a name=\"primal-from-dual-soft\"></a>\n",
    "$$$$\n",
    "\n",
    "Answer is in slide 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = get_primal_from_dual(alphas, X_train, y_train, C=C)\n",
    "plot_points_with_margin(X_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
